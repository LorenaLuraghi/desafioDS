{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093ad14a",
   "metadata": {},
   "source": [
    "### Ejercicio 5 - Exploración vs Explotación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7de299",
   "metadata": {},
   "source": [
    "El enfoque que se propone para atacar el problema planteado es el Aprendizaje por Recomensas. \n",
    "\n",
    "Nos encontramos frente a un problema dónde no es suficiente utilizar la información histórica disponible para resolverlo, ya que nos interesa que los usuarios interactuen con el sistema para decidir en que posiciones colocar que tipo de producto. Por lo tanto, es necesario consumir la información empírica obtenida a través de las interacciones del usuario al probar distintas posiciones.\n",
    "\n",
    "En este tipo de problemas es importante definir los elementos que van a estar involucrados a la hora del aprendizaje, algunos de ellos son: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c106a",
   "metadata": {},
   "source": [
    "- Agente: Es el encargado de interactuar con el sistema, en este caso el customer.\n",
    "- Entorno: El entorno es la posición de los productos en este caso, este se modifica a medida que el agente toma acciones. Lo podemos visualizar como una matriz (6,2) dónde cada elemento puede tomar el valor 0 si se va a explorar y 1 si se va a explotar.\n",
    "- Acción: La acción es llevada a cabo por el agente en función a las modificaciones que realiza el entorno. En este caso, la acción puede ser un click en un producto, una compra o cerrar la web.\n",
    "- Recompensa: La recompensa es una pieza clave porque es la herramienta para que el algoritmo aprenda. La debemos definir en función a los intereses, en este caso, lo que más nos interesa debería ser que el usuario termine comprando el producto, pero también puede ser valioso que explore otros productos (retardo) ya que esto nos podria derivar a comprar otro tipo de productos, entonces por ejemplos se podría distribuir de la siguiente forma:\n",
    "        - Hacer click en los productos: recompensa 1\n",
    "        - Comprar: recompensa 100\n",
    "        - Cerrrar ventana: -1\n",
    "- Episodio: Es \"una partida\" de juego, en este caso lo podemos visualizar como el proceso entre la búsqueda del primer producto por el user hasta que decide cerrar la web o comprar.\n",
    "\n",
    "\n",
    "\n",
    "El objetivo es construir una función de valor, es decir, estimar en función de las acciones que se tomen, cual va a ser la recompensa final (generar un impacto en la decisión de comprar/no comprar en función de los productos que se muestren al usuario) y encontrar la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66de40",
   "metadata": {},
   "source": [
    "La pregunta sobre cuanto explotar y cuanto explorar es clave en este tipo de aprendizaje, dado que no podemos poner el mismo foco en explorar y explotar al mismo tiempo. Si solo explotamos, es probable que nos estanquemos en un óptimo local y no tenga sentido para el negocio y si sólo exploramos va a ser una cuestión de suerte que usuario compre o no. \n",
    "Son diversas las técnicas que se han desarrollado para encontrar un balance entre la exploración y explotación.\n",
    "\n",
    "Uno de los métodos más comunes es $\\epsilon$-greedy dónde se debe definir una probabilidad a explorar, en función de las necesidades del sistema y luego se elige al azar si se desea explorar o explotar. En este caso, podriamos definir relaizar un sorteo para cada uno de los cuadrantes o simplemente sortear la cantidad de cuadrantes que se desea explorar y cuantos explotar. Son diversas las formas que se pueden elegir los distintos elementos del diseño de la solución. \n",
    "\n",
    "Otro de los métodos (softmax) consiste en asignarle mayor probabilidad las acciones con estimación más alta, en este caso podría ser una buena estrategia ya que el agoritmo va a ir aprendiendo en producción y no con simulaciones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2f59d",
   "metadata": {},
   "source": [
    "Si entrenamos el modelo de manera online, la interacción de los usuarios en cada episodio es clave para ir actualizando los parámetros de las funciones mencionadas en la parte anterior.\n",
    "\n",
    "Podemos definir varias métricas para evaluar el desempeño del sistema, sin embargo, al estar modificando los parámetrocs de manera online, es necesario revisar varios aspectos:\n",
    "\n",
    "- La evolución de las recompensas a lo largo de los distintos episodios (El usuario está comprando más?)\n",
    "- Convergencia de la función de valor. El sistema encontró una serie de pasos óptimos? O continua aprendiendo? \n",
    "- Tiempo de los episodios: Si bien al momento de definir las recompensas en función del negocio podemos evitar que esto ocurra, es clave entender qué es lo que está aprendiendo el sistema. Sabemos que clickear tiene recompensa, y si el sistema aprende a recomendar productos de manera que el usuario realice gran cantidad de clicks además de la compra? Se puede re-definir por ejemplo las recompensas y que click tenga valor nulo... \n",
    "- Es importante también definir métricas para monitorear a nivel negocio. Que montos se están comprando por recomendación? Qué tipo de productos? Es lo que deseamos? Funciona bien el sistema para distintas categorías o suele funcionar mejor cuando comienza la búsqueda de alguna manera en particular?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d6003",
   "metadata": {},
   "source": [
    "Cómo todos los enfoques, también existen desventajas a la hora llevarlos a cabo. En este caso, estamos frente a un enfoque que suele ser costoso por la cantidad de iteraciones necesarias y el aprendizaje continuo. Además, también el aprendizaje suele ser muy lento y no siempre está asegurado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f3fc0",
   "metadata": {},
   "source": [
    "Referencias:\n",
    "\n",
    "Curso Aprendizaje por Refuerzo FING: https://eva.fing.edu.uy/course/view.php?id=1231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8b492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
